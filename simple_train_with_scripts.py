#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆè®­ç»ƒè„šæœ¬ - ä½¿ç”¨äºŒåˆ†ä¹‹ä¸€æ¨ç†ä¸–ç•Œå®Œæ•´ç‰ˆ2ç›®å½•ä¸‹çš„å‰§æœ¬æ–‡ä»¶
æ­¤è„šæœ¬ç›´æ¥å¤„ç†æ–‡æœ¬æ•°æ®å¹¶æä¾›åŸºæœ¬åˆ†æï¼Œä¸ä¾èµ–DockeræœåŠ¡
"""

import os
import sys
import re
from collections import Counter

# é…ç½®è·¯å¾„
# SCRIPTS_DIR = r"c:\Users\11928\Desktop\linshi\1"
SCRIPTS_DIR = r"c:\Users\11928\Desktop\linshi\äºŒåˆ†ä¹‹ä¸€æ¨ç†ä¸–ç•Œå®Œæ•´ç‰ˆ2"
OUTPUT_DIR = r"c:\Users\11928\Desktop\linshi\output"
SCRIPTS_FILE = os.path.join(OUTPUT_DIR, "merged_scripts.txt")
MANUALS_FILE = os.path.join(OUTPUT_DIR, "merged_manuals.txt")


def check_and_process_docx_files():
    """æ£€æŸ¥å¹¶å¤„ç†DOCXæ–‡ä»¶"""
    print(f"ğŸ” æ£€æŸ¥ {SCRIPTS_DIR} ç›®å½•ä¸­çš„DOCXæ–‡ä»¶...")
    
    # æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(SCRIPTS_DIR):
        print(f"âŒ é”™è¯¯: æºç›®å½•ä¸å­˜åœ¨: {SCRIPTS_DIR}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰DOCXæ–‡ä»¶
    docx_files = [f for f in os.listdir(SCRIPTS_DIR) if f.endswith('.docx')]
    if not docx_files:
        print(f"âŒ é”™è¯¯: åœ¨ {SCRIPTS_DIR} ç›®å½•ä¸­æœªæ‰¾åˆ°DOCXæ–‡ä»¶")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(docx_files)} ä¸ªDOCXæ–‡ä»¶")
    for file in docx_files:
        print(f"   - {file}")
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
    if os.path.exists(SCRIPTS_FILE) and os.path.exists(MANUALS_FILE):
        print(f"\nâœ… æ£€æµ‹åˆ°å·²å¤„ç†çš„æ–‡æœ¬æ–‡ä»¶:")
        print(f"   - å‰§æœ¬æ–‡ä»¶: {SCRIPTS_FILE} ({os.path.getsize(SCRIPTS_FILE) / 1024:.2f} KB)")
        print(f"   - æ‰‹å†Œæ–‡ä»¶: {MANUALS_FILE} ({os.path.getsize(MANUALS_FILE) / 1024:.2f} KB)")
        return True
    
    # å¦‚æœæ²¡æœ‰å¤„ç†è¿‡ï¼Œæç¤ºè¿è¡Œå¤„ç†è„šæœ¬
    print(f"\nâŒ æœªæ£€æµ‹åˆ°å·²å¤„ç†çš„æ–‡æœ¬æ–‡ä»¶")
    print(f"   è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å¤„ç†DOCXæ–‡ä»¶:")
    print(f"   cd agent-project")
    print(f"   python process_docx_scripts.py")
    return False


def analyze_text_file(file_path, description):
    """åˆ†ææ–‡æœ¬æ–‡ä»¶å†…å®¹"""
    print(f"\nğŸ“Š åˆ†æ{description}æ–‡ä»¶: {os.path.basename(file_path)}")
    print("=" * 50)
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # åŸºæœ¬ç»Ÿè®¡
        char_count = len(text)
        word_count = len(text.split())
        line_count = len(text.split('\n'))
        paragraph_count = len([p for p in text.split('\n\n') if p.strip()])
        
        print(f"åŸºæœ¬ç»Ÿè®¡:")
        print(f"  - å­—ç¬¦æ•°: {char_count:,}")
        print(f"  - å•è¯æ•°: {word_count:,}")
        print(f"  - è¡Œæ•°: {line_count:,}")
        print(f"  - æ®µè½æ•°: {paragraph_count:,}")
        
        # æå–è§’è‰²åï¼ˆåŸºäºå¸¸è§çš„å‰§æœ¬æ ¼å¼ï¼‰
        # è§’è‰²åé€šå¸¸å•ç‹¬å ä¸€è¡Œï¼Œåé¢è·Ÿç€å¯¹è¯
        potential_characters = []
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            # ç®€å•çš„è§’è‰²åæ£€æµ‹ï¼šå…¨å¤§å†™ã€é•¿åº¦é€‚ä¸­ã€åé¢æœ‰ç©ºè¡Œæˆ–å¯¹è¯
            if (line.isupper() and 2 <= len(line) <= 20 and 
                not any(c.isdigit() for c in line) and
                i+1 < len(lines) and lines[i+1].strip()):
                potential_characters.append(line)
        
        # ç»Ÿè®¡è§’è‰²åå‡ºç°æ¬¡æ•°
        character_counts = Counter(potential_characters)
        top_characters = character_counts.most_common(10)
        
        if top_characters:
            print(f"\næ£€æµ‹åˆ°çš„è§’è‰²ï¼ˆå‰10åï¼‰:")
            for character, count in top_characters:
                print(f"  - {character}: {count} æ¬¡")
        
        # æå–å…³é”®è¯
        # ç®€å•çš„ä¸­æ–‡å…³é”®è¯æå–ï¼ˆæ’é™¤å¸¸è§åœç”¨è¯ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,}', text)
        common_stopwords = {'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æœ‰', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 'ä½ ', 'ä»¬', 'å°±', 'éƒ½'}
        filtered_words = [w for w in chinese_words if w not in common_stopwords and len(w) >= 2]
        word_counts = Counter(filtered_words)
        top_words = word_counts.most_common(15)
        
        if top_words:
            print(f"\nå…³é”®è¯ï¼ˆå‰15åï¼‰:")
            for word, count in top_words[:10]:
                print(f"  - {word}: {count} æ¬¡")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ®µè½ä½œä¸ºæ ·æœ¬
        print(f"\næ ·æœ¬å†…å®¹é¢„è§ˆ:")
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if paragraphs:
            sample_paragraph = paragraphs[0][:200] + ('...' if len(paragraphs[0]) > 200 else '')
            print(f"{sample_paragraph}")
        
        return True
    
    except Exception as e:
        print(f"âŒ åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return False


def create_local_training_index():
    """åˆ›å»ºæœ¬åœ°è®­ç»ƒç´¢å¼•æ–‡ä»¶"""
    print(f"\nğŸ”„ åˆ›å»ºæœ¬åœ°è®­ç»ƒç´¢å¼•...")
    
    try:
        # è¯»å–å‰§æœ¬å’Œæ‰‹å†Œå†…å®¹
        with open(SCRIPTS_FILE, 'r', encoding='utf-8') as f:
            scripts_text = f.read()
        
        with open(MANUALS_FILE, 'r', encoding='utf-8') as f:
            manuals_text = f.read()
        
        # åˆ›å»ºç®€å•çš„ç´¢å¼•æ–‡ä»¶
        index_content = f"""# å‰§æœ¬è®­ç»ƒæ•°æ®ç´¢å¼•

## åŸºæœ¬ä¿¡æ¯
- æºç›®å½•: {SCRIPTS_DIR}
- å‰§æœ¬æ–‡ä»¶å¤§å°: {os.path.getsize(SCRIPTS_FILE) / 1024:.2f} KB
- æ‰‹å†Œæ–‡ä»¶å¤§å°: {os.path.getsize(MANUALS_FILE) / 1024:.2f} KB
- ç´¢å¼•åˆ›å»ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®ç»Ÿè®¡
- å‰§æœ¬å­—ç¬¦æ•°: {len(scripts_text):,}
- å‰§æœ¬å•è¯æ•°: {len(scripts_text.split()):,}
- æ‰‹å†Œå­—ç¬¦æ•°: {len(manuals_text):,}
- æ‰‹å†Œå•è¯æ•°: {len(manuals_text.split()):,}

## ä½¿ç”¨è¯´æ˜
1. è¿™äº›æ–‡æœ¬æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºè®­ç»ƒ
2. æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®æˆ–è¿›ä¸€æ­¥å¤„ç†
3. å½“DockeræœåŠ¡å¯ç”¨æ—¶ï¼Œå¯ä»¥å¯¼å…¥åˆ°å®Œæ•´ç³»ç»Ÿä¸­
"""
        
        index_file = os.path.join(OUTPUT_DIR, "training_index.md")
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(index_content)
        
        print(f"âœ… æœ¬åœ°è®­ç»ƒç´¢å¼•å·²åˆ›å»º: {index_file}")
        return True
    
    except Exception as e:
        print(f"âŒ åˆ›å»ºç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
        return False


def show_usage_guide():
    """æ˜¾ç¤ºä½¿ç”¨æŒ‡å—"""
    print("\nğŸ“– ä½¿ç”¨æŒ‡å—")
    print("=" * 50)
    print("è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥:")
    print("1. æŸ¥çœ‹åˆ†æç»“æœå’Œç»Ÿè®¡æ•°æ®")
    print("2. ç›´æ¥ä½¿ç”¨outputç›®å½•ä¸­çš„æ–‡æœ¬æ–‡ä»¶è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†")
    print("3. å½“ç½‘ç»œç¯å¢ƒæ”¹å–„æ—¶ï¼Œé‡æ–°å¯åŠ¨DockeræœåŠ¡è¿›è¡Œå®Œæ•´è®­ç»ƒ")
    print("\nå¯ç”¨çš„æ–‡æœ¬æ–‡ä»¶:")
    print(f"   - {SCRIPTS_FILE}")
    print(f"   - {MANUALS_FILE}")
    print(f"   - {os.path.join(OUTPUT_DIR, 'training_index.md')}")
    print("=" * 50)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä½¿ç”¨å‰§æœ¬æ•°æ®è¿›è¡Œç®€åŒ–è®­ç»ƒ")
    print(f"ğŸ“‚ æºæ–‡ä»¶å¤¹: {SCRIPTS_DIR}")
    print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶å¤¹: {OUTPUT_DIR}")
    
    # æ£€æŸ¥å¹¶å¤„ç†DOCXæ–‡ä»¶
    if not check_and_process_docx_files():
        return
    
    # åˆ†æå‰§æœ¬æ–‡ä»¶
    analyze_text_file(SCRIPTS_FILE, "å‰§æœ¬")
    
    # åˆ†ææ‰‹å†Œæ–‡ä»¶
    analyze_text_file(MANUALS_FILE, "æ‰‹å†Œ")
    
    # åˆ›å»ºæœ¬åœ°è®­ç»ƒç´¢å¼•
    create_local_training_index()
    
    # æ˜¾ç¤ºä½¿ç”¨æŒ‡å—
    show_usage_guide()
    
    print("\nğŸ‰ ç®€åŒ–è®­ç»ƒå®Œæˆï¼æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„æ–‡æœ¬æ–‡ä»¶è¿›è¡Œè¿›ä¸€æ­¥åˆ†ææˆ–è®­ç»ƒã€‚")


if __name__ == "__main__":
    import time
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œæ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ‘‹ è„šæœ¬æ‰§è¡Œç»“æŸ")